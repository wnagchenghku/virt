1. lightvm can greatly benefit our VM fault tolerance system PLOVER.


lightvm can benefit the memory footprint in PLOVER. The background processes in kernel can contribute around one thousand dirty memory pages in virtual machine. With lightvm, this part of memory footprint can be further reduced.

Besides, I think the fast pause/resume of lightvm is an exciting feature to PLOVER. We found the basic virtual machine fault tolerance system carried in QEMU spends a lot of time pausing the VM, and the reason is when QEMU issues vcpu related system calls, the KVM code in kernel will always do the operation of tracking the pid of the task that runs the vcpu first and this causes time cost due to synchronization if one CPU is modified by another CPU. Since kernel tracks the pid only for the purpose of yielding, so actually we only need to track the pid only when QEMU issues the vcpu running system call (KVM_SET_REGS, KVM_RUN).


This bug has been fixed on ubuntu 16, but we were using ubuntu 14. Therefore, it takes us a lot of time to find the root cause. With lightvm, I think this kind of problem can be mitigated.


2. What do you know about Xen?


I have only played with the Xen fault tolerance systems including  COLO, which is a virtual machine fault tolerance system developed by Huawei, and Remus. So, I am quite familiar with the virtual machine migration protocol in Xen, but for the internal mechanism of Xen, I may need to have a deep understanding. And that's why I applied for your internship because I am expecting to have a deeper understanding on virtualization technology.


3. Tell me about your research.


I was previously working on virtual machine fault tolerance systems, PLOVER. PLOVER can efficiently replicate virtual machine with strong fault tolerance guarantee. Specifically, it models the virtual machine as state machine by running the primary virtual machine and backup virtual machine in parallel and ensuring both virtual machines receive the same total order of network inputs.


Because the primary and backup run the same code, receive the same network inputs, their memory states should be almost the same and accordingly the amount of data to be transferred for memory state synchronization is very small.


To achieve strong fault tolerance guarantee, it adds a witness machine to form a Paxos group.


The main idea is what I have mentioned above. But there are some practical challenges in PLOVER, like how to achieve network input consensus in the hypervisor without hurting the virtual machine's performance and how to maximize the same dirty memory pages between two VMs, etc.


4. Why do you apply for NEC


I applied for NEC for two reasons. I was attracted by the great works like lightvm at the top conferences. I was previously working the virtual machine fault tolerance projects, but with the focus on virtual machine migration technologies and distributed consensus protocols which are mainly on top of the virtual machine itself, therefore I am expecting to come to NEC Europe to get a deeper understanding on the internal mechanism of virtual machine.


The second reason is that I am actually planning the whole career of my life. I spent half a year working as an intern in Hamburg three years ago, and I really enjoyed that time. After that I have made up my mind to immigrate to Germany and find a job there after graduation. So if I am qualified, I will join NEC as a researcher after my graduation.


5. Duration of internship


I think this is really negotiable. I have told this internship to my supervisor and he really values this opportunity and wants me to go there. He said if I am working on like an NSDI or Eurosys paper whose deadlines are in September or October, then my internship duration can be extended to the end of October.


6. container is less isolated than VM


To complicate things matter, any container that can monopolize or exhaust system resources (e.g., memory, file descriptors, user IDs, forkbombs) will cause a DoS attack on all other containers on that host.


forkbomb: fork炸弹以极快的速度创建大量进程（进程数呈以2为底数的指数增长趋势），并以此消耗系统分配予进程的可用空间使进程表饱和，而系统在进程表饱和后就无法运行新程序，除非进程表中的某一进程终止；但由于fork炸弹程序所创建的所有实例都会不断探测空缺的进程槽并尝试取用以创建新进程，因而即使在某进程终止后也基本不可能运行新进程。fork炸弹生成的子程序在消耗进程表空间的同时也会占用CPU和内存，从而导致系统与现有进程运行速度放缓，响应时间也会随之大幅增加，以致于无法正常完成任务，从而使系统的正常运作受到严重影响。


All containers share the kernel and its API. Kernel API are difficult to secure. One container uses the system call which is not protected securely, the whole system is in danger.

7. Use cases of of lightvm

There are a bunch of cells connected to the core network. Cells connected to the mobile devices.

Let's take some processing close to the cells (mobile edge cloud). We can offload some functionality from the mobile to the cells. Everything you offload has its own functionality.

One firewall for each mobile in the cell

The Mobile Edge Computing is an ideal place to instantiate personal firewalls for mobile users. The difficulty is that the amount of deployed hardware at any single cell is very limited (one or a few machines), while the number of active users in the cell is on the order of a few thousand. Moreover, users enter and leave the cell continuously, so it is critical to be able to instantiate, terminate and migrate personal firewalls quickly and cheaply, following the user through the mobile network.

we can run as many as 8000 such firewalls on our 64-core AMD machine.

8. migration in lightvm

With this in place, migration begins by chaos opening a TCP connection to a migration daemon running on the remote host and by sending the guest’s configuration so that the daemon pre-creates the domain and creates the devices.

Next, to suspend the guest, chaos issues an ioctl to the sysctl back-end, which will set a field in the shared page to denote that the shutdown reason is suspend, and triggers the event channel. The front-end will receive the request to shutdown, upon which the guest will save its internal state and unbind noxs-related event channels and device pages.

Once the guest is suspended we rely on libxc code to send the guest data to the remote host.

9. What about vMigrater?

In the normal case, when the disk I/O job request finishes, the program will do some simple computation and then issue the next I/O request.

However, when there is mixed computation and disk I/O on a virtual CPU, the computation part will use up the time slice of the CPU, causing the virtual CPU to be de-scheduled by the hypervisor. This leads to a severe I/O inactivity in virtual machine because after the vCPU is de-scheduled, the I/O jobs running on this vCPU cannot be handled in time.

What makes this worse is that this I/O inactivity problem causes the I/O scheduler running in the host machine work poorly. The Completely Fair Queuing (CFQ) I/O Scheduler is currently the default I/O scheduler in Linux. With CFQ, each process is assigned its own queue, and each queue is assigned timeslice. The I/O scheduler visits each queue in a round-robin fashion, servicing requests from the queue until the queue's timeslice is exhausted, or until no more requests remain. In the latter case, the CFQ I/O Scheduler will then idle for a brief period - by default, 10ms - waiting for a new request on the queue. If the anticipation pays off, the I/O scheduler avoids seeking. If not, the waiting was in vain, and the scheduler moves on to the next process's queue.

However, since in the I/O inactivity scenario, the virtual CPU running the I/O job has been de-scheduled so that the next I/O request cannot be issued, the CFQ I/O scheduler will always waste time in waiting.

The basic idea vMigrater is to migrate the thread running the mixed computation and I/O workload to an available virtual CPU when it has almost finished using up its time slice.

